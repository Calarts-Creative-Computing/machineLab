# layers
2 8 8 1 
# activation functions (0=Linear, 1=Sigmoid, 2=ReLU, 3=Tanh, 4=Softmax)
1 1 0 
# weights
0.128631 0.940201 
0.980172 0.0641888 
0.838332 0.795285 
0.190987 0.0744964 
0.757684 0.943644 
0.12869 0.805199 
0.0902848 0.31053 
0.23435 0.202103 
-0.478782 -0.346187 -0.833389 -0.192333 -0.950136 -0.245007 -0.17349 -1.05255 
-0.623747 -0.733624 -0.662202 -0.52122 -0.275636 -0.778763 -0.647297 -0.584178 
-0.497021 -0.940336 -0.313094 -0.422322 -0.063093 -0.236097 -1.02852 -0.973063 
-0.242228 -0.252127 -0.947055 -0.766045 -0.198955 -0.962289 -0.2115 -0.195635 
-0.00103196 -0.161942 -0.846066 -0.715302 -0.620204 -0.768517 -0.689377 -0.208439 
-0.452976 -0.180127 -0.00531907 -0.724341 -0.979772 -0.950046 -0.468568 -0.537047 
-0.280993 -0.594516 -0.637922 -0.714027 -0.507163 -0.249075 -0.817363 -0.902122 
-0.854724 -0.351839 -0.178573 -0.295647 -0.506108 -0.234906 -0.536911 -0.961506 
0.0134943 0.0171854 0.0156877 0.0101742 0.0145543 0.0202081 0.0172705 0.0171169 
# biases
0.898524 0.0536016 0.379446 0.0947061 0.380111 0.549407 0.507738 0.605099 
-0.537458 0.00240222 -0.344656 -1.02359 -0.809677 -0.523987 -0.112058 -0.893512 
2.16464 
