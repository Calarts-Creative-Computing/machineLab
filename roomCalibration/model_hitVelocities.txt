# layers
2 5 5 1 
# activation functions (0=Linear, 1=Sigmoid, 2=ReLU, 3=Tanh, 4=Softmax)
1 1 0 
# weights
0.453781 0.170736 
0.946707 0.329662 
0.356529 0.0976241 
0.616011 0.747345 
0.998755 0.961777 
-0.773225 -0.173515 -0.657815 -0.695608 -0.685235 
-0.523485 -1.11946 -0.181981 -0.539453 -1.03368 
-0.747698 -0.311661 -0.627786 -0.911199 -0.976144 
-0.390719 -0.468502 -0.429138 -0.346544 -0.953203 
-0.364376 -0.339976 -0.759344 -0.741015 -1.04864 
0.0951708 0.0853407 0.0889726 0.0597099 0.0837496 
# biases
0.770937 0.334431 0.580671 0.286266 0.842203 
-0.72718 -0.277571 -0.127861 -1.01534 -0.435684 
2.97163 
