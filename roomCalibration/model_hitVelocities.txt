# layers
2 8 8 1 
# activation functions (0=Linear, 1=Sigmoid, 2=ReLU, 3=Tanh, 4=Softmax)
1 1 0 
# weights
-6.38384 3.33148 
-1.23235 2.24668 
-2.33423 3.73192 
-1.75397 2.65966 
-1.53884 2.78161 
0.103333 -0.0495106 
-0.960466 1.108 
-1.30732 1.93461 
1.70693 0.162602 0.678648 0.431372 0.271127 -0.0434694 -0.0827081 0.485484 
0.53494 -0.0722551 0.809991 0.450455 0.264625 -0.106206 0.385037 0.626983 
1.57948 0.31284 1.12134 0.586195 0.526342 -0.516356 0.0320573 0.232557 
0.727539 0.762529 0.864301 0.118735 0.740579 0.4891 0.206209 -0.0264263 
0.291263 0.988185 0.912245 0.45097 0.928668 1.01289 0.628257 0.363232 
0.799345 0.396 0.463549 0.892454 0.205656 0.544515 0.52599 -0.0709009 
0.0930319 0.757306 0.785967 0.284192 0.523793 0.834446 0.266381 0.317838 
1.75474 0.60954 0.743007 0.498696 0.799165 -0.420804 0.178249 0.172397 
1.98945 0.642913 2.14506 0.0760463 -1.00334 -0.0253513 -0.193084 2.14581 
# biases
2.57442 -1.69856 -2.51422 -1.43786 -1.89091 -0.455688 -0.539986 -0.992433 
-1.01027 -0.18764 -0.97069 0.606142 0.879958 0.725844 0.571234 -1.16393 
-1.82769 
