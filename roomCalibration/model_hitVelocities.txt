# layers
2 5 5 1 
# activation functions (0=Linear, 1=Sigmoid, 2=ReLU, 3=Tanh, 4=Softmax)
1 1 0 
# weights
0.46119 0.267364 
0.273106 0.312272 
0.455206 0.142523 
0.748612 0.251134 
0.564243 0.601936 
-0.486361 -0.944478 -0.415294 -0.169415 -0.756593 
-0.257789 -0.66165 -1.03013 -0.137422 -0.559887 
-0.355904 -0.436649 -0.685897 -0.462414 -1.03822 
-0.894109 -0.534626 -0.608138 -0.621751 -0.302115 
-0.538048 -0.321331 -0.926356 -0.742457 -0.623871 
0.0859344 0.0741396 0.0647083 0.0665175 0.0825288 
# biases
0.346921 0.551458 0.621015 0.0918901 0.0458621 
-0.86288 -0.978061 -0.58882 -0.663149 -0.471657 
2.748 
