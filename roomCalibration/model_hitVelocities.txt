# layers
2 8 8 1 
# activation functions (0=Linear, 1=Sigmoid, 2=ReLU, 3=Tanh, 4=Softmax)
1 1 0 
# weights
-1.91996 2.3621 
-3.32687 3.72405 
-2.79951 3.18944 
-1.63714 2.31708 
-2.91781 3.4234 
-1.46859 1.91353 
-0.848945 1.44523 
-1.21183 1.32451 
0.610796 0.0265094 0.857482 0.70653 0.549514 -0.0663874 -0.0293212 0.0225459 
0.26167 0.722004 0.529702 0.65455 0.4516 0.412124 -0.071092 -0.0312517 
0.854595 0.93815 0.707454 0.84792 0.939789 0.446864 0.155245 0.55335 
0.814529 0.56605 0.539738 0.528484 0.115319 0.634057 0.398592 0.978355 
0.28955 0.4385 0.459768 0.81903 -0.0277123 0.777885 0.393847 -0.166814 
0.724223 1.30644 1.01871 0.0610772 1.21735 0.343903 0.0248708 0.287452 
0.643292 1.30697 0.896581 0.10202 1.11015 0.258263 0.133413 0.374446 
0.25829 0.696513 0.280926 0.810586 0.681332 0.0262105 0.209259 0.568259 
-0.0494953 0.997145 -0.161899 -0.605257 0.394502 1.86986 1.85134 1.0681 
# biases
-1.15621 -0.836462 -1.02132 -1.5534 -1.24294 -1.09203 -0.584624 -0.642219 
0.569912 -0.532541 0.509201 0.566984 0.0226286 -0.859033 -0.918992 -0.872606 
-1.61245 
