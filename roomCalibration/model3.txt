# layers
2 5 5 1 
# activation functions (0=Linear, 1=Sigmoid, 2=ReLU, 3=Tanh, 4=Softmax)
1 1 0 
# weights
0.873816 0.432009 
0.208357 0.389001 
0.0572258 0.579991 
0.733263 0.825771 
0.524008 0.114363 
-0.372144 -0.93234 -0.0737178 -0.935171 -0.855668 
-0.609307 -0.969483 -0.220824 -0.543947 -0.197489 
-0.902026 -0.55616 -0.520046 -0.424656 -0.112673 
-0.754658 -0.556077 -0.406231 -0.717407 -0.651254 
-0.961966 -0.282905 -0.639774 -0.733404 -0.0827945 
0.127708 0.116316 0.105569 0.120981 0.0855199 
# biases
0.206921 0.212382 0.0291044 0.588069 0.487166 
-0.158146 -0.753844 -0.81102 -0.24401 -0.408654 
2.88634 
