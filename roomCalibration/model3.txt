# layers
2 5 5 1 
# activation functions (0=Linear, 1=Sigmoid, 2=ReLU, 3=Tanh, 4=Softmax)
1 1 0 
# weights
0.430217 0.71812 
0.359033 0.312989 
0.071689 0.349809 
0.910665 0.587332 
0.17863 0.974192 
0.569714 0.435268 0.600608 0.302337 0.821336 
0.788951 0.179135 0.78705 0.165085 0.30272 
0.0780467 0.800698 0.518207 0.392957 0.0818631 
0.761119 0.862779 0.0827828 0.0935404 0.470325 
0.93387 0.780454 0.316987 0.186122 0.876303 
-0.322606 0.432472 0.498347 -0.439031 0.426343 
# biases
0.141382 0.301816 0.820567 0.701104 0.45696 
0.211677 0.658167 0.284687 0.69794 0.779381 
-0.37315 
