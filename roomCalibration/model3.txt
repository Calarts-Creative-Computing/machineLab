# layers
2 5 5 1 
# activation functions (0=Linear, 1=Sigmoid, 2=ReLU, 3=Tanh, 4=Softmax)
1 1 0 
# weights
0.221037 0.0893458 
0.336319 0.307795 
0.900532 0.735843 
0.506218 0.342215 
0.596409 0.382722 
-0.572375 -0.504338 -0.157165 -0.4282 -0.735456 
-0.181972 -0.143636 -0.826701 -0.31995 -0.924808 
-0.871007 -0.172969 -0.703508 -0.416416 -0.443712 
-0.81484 -0.450015 -0.478586 -0.277145 -0.537755 
-0.153216 -0.626986 -0.46943 -0.301706 -0.824457 
0.14177 0.150265 0.108253 0.162191 0.137515 
# biases
0.420292 0.0205794 0.186791 0.609993 0.819641 
-0.571541 -0.695873 -0.168286 -0.557016 -0.690952 
3.48599 
