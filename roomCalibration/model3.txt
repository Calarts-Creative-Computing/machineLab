# layers
2 5 5 1 
# activation functions (0=Linear, 1=Sigmoid, 2=ReLU, 3=Tanh, 4=Softmax)
1 1 0 
# weights
0.0736855 0.507255 
0.69573 0.925244 
0.619738 0.38392 
0.639125 0.280233 
0.108514 0.590961 
-0.703942 -0.829963 -0.328449 -0.533788 -0.551444 
-0.876232 -0.789473 -0.0748914 -0.182964 -0.808504 
-0.905402 -0.207082 -0.575313 -0.678545 -0.287318 
-0.656629 -0.33292 -0.659938 -0.459219 -0.637697 
-0.703744 -0.718528 -0.645557 -0.446927 -0.127708 
0.118447 0.110364 0.120765 0.10673 0.0813051 
# biases
0.396452 0.582104 0.275129 0.254198 0.608602 
-0.377927 -0.599531 -0.678619 -0.594825 -0.530056 
2.89645 
