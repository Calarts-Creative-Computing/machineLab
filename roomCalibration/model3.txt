# layers
2 5 5 1 
# activation functions (0=Linear, 1=Sigmoid, 2=ReLU, 3=Tanh, 4=Softmax)
1 1 0 
# weights
0.493837 0.421257 
0.0937999 0.36939 
0.564962 0.834659 
0.181137 0.517429 
0.980835 0.642839 
0.267336 0.336485 0.415607 -0.0203386 0.5491 
0.881123 0.657835 0.15701 0.396743 0.645112 
0.851277 0.23036 0.134617 0.539426 0.717995 
0.816853 0.121139 0.593069 0.750145 0.990957 
0.472554 0.104032 0.403793 0.614624 0.0638848 
-0.037435 0.0958956 -0.22167 -0.388151 0.294567 
# biases
0.277231 0.0254343 0.870467 0.584431 0.0969828 
0.626496 0.217145 0.22737 0.0325117 0.748964 
0.423657 
